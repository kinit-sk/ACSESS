{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import argparse\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "import gc\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from textwrap import wrap\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "params = {'legend.fontsize': 'xx-large',\n",
    "         'axes.labelsize': 'xx-large',\n",
    "         'axes.titlesize':'xx-large',\n",
    "         'xtick.labelsize':'xx-large',\n",
    "         'ytick.labelsize':'xx-large'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, auc, roc_curve, roc_auc_score\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactSheetErrors:\n",
    "    def FileError(self,file):\n",
    "        raise IOError('[-] File Not Found : '+file)\n",
    "        exit\n",
    "    def DirectoryError(self,directory):\n",
    "        raise IOError('[-] Directory Not Found : '+directory)\n",
    "        exit\n",
    "    def ColumnError(self,column):\n",
    "        raise ValueError('[-] Column Not Found : '+column)\n",
    "        exit\n",
    "    def DebugError(self):\n",
    "        raise ValueError('[-] Debug Super Categories are None')\n",
    "        exit\n",
    "    def DebugInvalidSCError(self,debug_super_category):\n",
    "        raise ValueError('[-] Debug Invalid Super Category : '+debug_super_category)\n",
    "        exit\n",
    "err = FactSheetErrors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='padding:5px 5px 5px 5px; margin:15px 0px 0px 0px; background-color:red;'></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the dataset which contains images, labels.csv and info.json\n",
    "DATASET_PATH = './data_set'\n",
    "\n",
    "\n",
    "# Path of the directory where the results of this experiments will bee saved\n",
    "# logs.txt, super_categories.txt and one directory for each super_category\n",
    "# which contains categories.txt, categories_auc.txt, logs.txt, some figures etc \n",
    "PREDICTIONS_PATH = \"./experiment_1_results\"\n",
    "\n",
    "# number of categories to combine to make a super-category or a classification task\n",
    "CATEGORIES_TO_COMBINE = 5\n",
    "\n",
    "# number of images per category\n",
    "# shots = IMAGES_PER_CATEGORY/2\n",
    "IMAGES_PER_CATEGORY = 20\n",
    "\n",
    "# maximum limit on episodes/super-categories\n",
    "# can be none or an integer\n",
    "MAX_EPISODES = None\n",
    "\n",
    "# Normalize the input images according to the way neural networks were pretrained on ImageNet\n",
    "USE_NORMALIZATION = False\n",
    "\n",
    "# To generate an imagesheet : a pdf document with all the images per category/class\n",
    "GENERATE_IMAGESHEET = False\n",
    "\n",
    "# Debug Flag\n",
    "DEBUG_MODE = False\n",
    "\n",
    "# Debug Categories\n",
    "DEBUG_SUPER_CATEGORIES = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='padding:5px 5px 5px 5px; margin:15px 0px 0px 0px; background-color:red;'></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True super Categories\n",
    "TRUE_SUPER_CATEGORIES = None\n",
    "\n",
    "# seed for generating super-categories by the same random combination of categories\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the json.info file\n",
    "JSON_PATH = os.path.join(DATASET_PATH, \"info.json\")\n",
    "\n",
    "# Path of the CSV file which contains the label and image name\n",
    "CSV_PATH = os.path.join(DATASET_PATH, \"labels.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Prediction Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PREDICTIONS_PATH):\n",
    "    os.makedirs(PREDICTIONS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Directories and Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Directory\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    err.DirectoryError(DATASET_PATH)\n",
    "    \n",
    "#Check JSON file\n",
    "if not os.path.isfile(JSON_PATH):\n",
    "    err.FileError(JSON_PATH)\n",
    "\n",
    "# Check CSV File\n",
    "if not os.path.isfile(CSV_PATH):\n",
    "    err.FileError(CSV_PATH)\n",
    "\n",
    "# Check Predictions Directory\n",
    "if not os.path.exists(PREDICTIONS_PATH):\n",
    "    err.DirectoryError(PREDICTIONS_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open (JSON_PATH, \"r\")\n",
    "info = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True if CSV is tab separated otherwise false\n",
    "CSV_WITH_TAB = info[\"csv_with_tab\"]\n",
    "\n",
    "# Path of the directory where images to be used in this experiement are saved\n",
    "if info[\"images_in_sub_folder\"]:\n",
    "    IMAGE_PATH = os.path.join(DATASET_PATH, \"images\")\n",
    "else:  \n",
    "    IMAGE_PATH = DATASET_PATH\n",
    "\n",
    "\n",
    "\n",
    "# category column name in csv\n",
    "CATEGORY_COLUMN = info[\"category_column_name\"]\n",
    "\n",
    "# Super category column name in csv\n",
    "SUPER_CATEGORY_COLUMN = info[\"super_category_column_name\"]\n",
    "\n",
    "\n",
    "# image column name in csv\n",
    "IMAGE_COLUMN = info[\"image_column_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check True Super Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE_SUPER_CATEGORIES = info[\"has_super_categories\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0_start = time.time()\n",
    "timestamp = str(datetime.datetime.utcnow().strftime(\"%Y%m%d_%H%M%S_%f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if CSV_WITH_TAB:\n",
    "    data = pd.read_csv(CSV_PATH, sep=\"\\t\", encoding=\"utf-8\") \n",
    "else:\n",
    "    data = pd.read_csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Shape : \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Debug Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG_MODE:\n",
    "    if DEBUG_SUPER_CATEGORIES is None:\n",
    "        err.DebugError()\n",
    "    else:\n",
    "        DEBUG_SUPER_CATEGORIES = [s_c.strip() for s_c in DEBUG_SUPER_CATEGORIES.split(',')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Categoris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = data[CATEGORY_COLUMN].unique()\n",
    "total_categories = len(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRUE_SUPER_CATEGORIES:\n",
    "    super_categories = data[SUPER_CATEGORY_COLUMN].value_counts().index.values\n",
    "    iterations_needed = None\n",
    "else:\n",
    "    random.Random(SEED).shuffle(categories)\n",
    "    iterations_needed = math.ceil(total_categories/CATEGORIES_TO_COMBINE)\n",
    "    \n",
    "    print(\"Iterations required : \", iterations_needed)\n",
    "    print(\"Categories to combine togather : \", CATEGORIES_TO_COMBINE)\n",
    "    super_categories = np.array_split(categories,iterations_needed)\n",
    "    \n",
    "total_super_categories = len(super_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Super-Categories : \", total_super_categories)\n",
    "print(\"Total Categories/Classes : \", total_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Debug Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG_MODE:\n",
    "    filtered_super_categories = []\n",
    "    for s_c in DEBUG_SUPER_CATEGORIES:\n",
    "\n",
    "        if TRUE_SUPER_CATEGORIES:\n",
    "            if s_c in super_categories:\n",
    "                filtered_super_categories.append(s_c)\n",
    "            else:\n",
    "                err.DebugInvalidSCError(s_c)\n",
    "\n",
    "        else:\n",
    "            if int(s_c) in range(0,total_super_categories):\n",
    "                filtered_super_categories.append(int(s_c))\n",
    "            else:\n",
    "                err.DebugInvalidSCError(s_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Super_Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def prepare_single_super_data(index, super_category):\n",
    "    super_dict = {}\n",
    "    \n",
    "    if TRUE_SUPER_CATEGORIES:\n",
    "        super_category_df = data[data[SUPER_CATEGORY_COLUMN] == super_category].groupby(CATEGORY_COLUMN).sample(n=IMAGES_PER_CATEGORY, random_state=SEED)\n",
    "        super_dict['super_category'] = super_category\n",
    "    else:\n",
    "        super_category_df = data[data[CATEGORY_COLUMN].isin(super_category)].groupby(CATEGORY_COLUMN).sample(n=IMAGES_PER_CATEGORY, random_state=SEED)\n",
    "        super_dict['super_category'] = str(index)\n",
    "    \n",
    "    \n",
    "    super_category_df['label_cat'] = super_category_df[CATEGORY_COLUMN].astype('category')\n",
    "    \n",
    "    \n",
    "    super_dict['categories'] = super_category_df['label_cat'].cat.categories.values\n",
    "    super_dict['images'] = super_category_df[CATEGORY_COLUMN].value_counts().values\n",
    "    \n",
    "    \n",
    "    train_data, valid_data = train_test_split(\n",
    "        super_category_df, test_size=0.5, \n",
    "        random_state=420, shuffle=True, \n",
    "        stratify=super_category_df[CATEGORY_COLUMN]\n",
    "    )\n",
    "    \n",
    "\n",
    "    \n",
    "    super_dict['train_labels'] = train_data[CATEGORY_COLUMN].values\n",
    "    super_dict['valid_labels'] = valid_data[CATEGORY_COLUMN].values\n",
    "    \n",
    "    super_dict['train_labels_num'] =  train_data['label_cat'].cat.codes.values\n",
    "    super_dict['valid_labels_num'] = valid_data['label_cat'].cat.codes.values\n",
    "    \n",
    "    \n",
    "    super_dict['train_data'] = train_data[IMAGE_COLUMN].values\n",
    "    super_dict['valid_data'] = valid_data[IMAGE_COLUMN].values\n",
    "    \n",
    "    \n",
    "    return super_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_data = []\n",
    "for index, super_category in enumerate(super_categories):\n",
    "    #Debug Mode\n",
    "    if DEBUG_MODE:\n",
    "        if TRUE_SUPER_CATEGORIES: \n",
    "            if super_category in filtered_super_categories:\n",
    "                super_data.append(prepare_single_super_data(index, super_category))\n",
    "        else:\n",
    "            if index in filtered_super_categories:\n",
    "                super_data.append(prepare_single_super_data(index, super_category))   \n",
    "                \n",
    "    # Not Debug Mode\n",
    "    else:\n",
    "        super_data.append(prepare_single_super_data(index, super_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, dataset_images, dataset_labels, transform):\n",
    "\n",
    "        # Transforms\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.images = dataset_images\n",
    "        self.labels = dataset_labels\n",
    "\n",
    "        self.data_len = len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \n",
    "        single_img = self.images[index]\n",
    "        img_transformed = torch.from_numpy(single_img).long()\n",
    "        img_transformed = img_transformed.permute(2, 0, 1)\n",
    "        img_transformed = torch.from_numpy(np.array(img_transformed)).float() / 255.\n",
    "\n",
    "        \n",
    "        single_label = self.labels[index]\n",
    "        single_label = single_label.astype(np.compat.long)\n",
    "        \n",
    "        return img_transformed, single_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(super_data_set, batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    if not USE_NORMALIZATION:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    \n",
    "    train_ds=ImgDataset(super_data_set['train_images'],super_data_set['train_labels_num'], transform)\n",
    "    valid_ds=ImgDataset(super_data_set['valid_images'],super_data_set['valid_labels_num'], transform)\n",
    "    \n",
    "    print(\"############################################\")\n",
    "    print(\"============================================\")\n",
    "    print(\"=== Super-Category: \", super_data_set['super_category'])\n",
    "    print(\"============================================\")\n",
    "    print(\"############################################\")\n",
    "    print()\n",
    "    print(\"Total Categories: \", len(super_data_set['categories']))\n",
    "    print(\"Total Images: \", super_data_set['images'].sum())\n",
    "    print(\"Train Data: \", len(train_ds))\n",
    "    print(\"Validation Data: \", len(valid_ds))\n",
    "    print()\n",
    "    \n",
    "    data_stats = {\n",
    "        \"total_images\" : super_data_set['images'].sum(),\n",
    "        \"train_images\" : len(train_ds),\n",
    "        \"valid_images\" : len(valid_ds)\n",
    "    }\n",
    "    \n",
    "    \n",
    "    dataloaders = {\n",
    "        'val':DataLoader(\n",
    "            valid_ds, \n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "        ),\n",
    "        'train':DataLoader(\n",
    "            train_ds, \n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    }\n",
    "\n",
    "    dataset_sizes = {\n",
    "        'val': len(valid_ds),\n",
    "        'train':len(train_ds)\n",
    "    }\n",
    "    \n",
    "    return dataloaders, dataset_sizes, data_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(only_train_last_layer=True, number_of_classes=2):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "\n",
    "    if only_train_last_layer:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, number_of_classes)\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=5):\n",
    "    \n",
    "    \n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"Training\")\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_train_acc = 0.0\n",
    "    \n",
    "    loss_history = []\n",
    "    score_history = []\n",
    "    \n",
    "    \n",
    "    train_loss, train_score, valid_loss, valid_score = [], [], [], []\n",
    "\n",
    "    print(\"Epoch: \", end=\" \")\n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch, end=\" \")\n",
    "        \n",
    "        train_predictions, train_ground, valid_predictions, valid_ground = [], [], [], []\n",
    "        train_predicted_probabilities, valid_predicted_probabilities = [],[]\n",
    "        \n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    \n",
    "#                     inputs = inputs.permute(0, 3, 1, 2)\n",
    "#                     inputs = torch.from_numpy(np.array(inputs)).float() / 255.\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    probabilities = F.softmax(outputs, dim=1)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                 \n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                # saving prediction and ground truth for future use\n",
    "                if phase == 'train':\n",
    "                    train_predictions += list(preds.numpy())\n",
    "                    train_ground += list(labels.numpy())\n",
    "                    train_predicted_probabilities += list(probabilities.detach().numpy())\n",
    "                else:\n",
    "                    valid_predictions += list(preds.numpy())\n",
    "                    valid_ground += list(labels.numpy())\n",
    "                    valid_predicted_probabilities += list(probabilities.detach().numpy())\n",
    "                \n",
    "                \n",
    "            # end dataloader loop\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "#                 loss_history.append(running_loss)\n",
    "#                 score_history.append(running_corrects)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = (running_corrects.double() / dataset_sizes[phase]).item()\n",
    "\n",
    "#             print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            loss_history, score_history = (train_loss, train_score) if phase == 'train' else (valid_loss, valid_score)\n",
    "            loss_history.append(epoch_loss)\n",
    "            score_history.append(epoch_acc)\n",
    "\n",
    "            \n",
    "            if phase == 'train' and epoch_acc > best_train_acc:\n",
    "                 best_train_acc = epoch_acc\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_val_acc:\n",
    "                best_val_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        # end phase loop\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    training_time = '{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)\n",
    "    \n",
    "    print('Training complete in: {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best train Acc: {:.2f}'.format(best_train_acc))\n",
    "    print('Best val Acc: {:.2f}'.format(best_val_acc))\n",
    "    print()\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return dict(   \n",
    "        model = model,\n",
    "        train_loss = train_loss,\n",
    "        train_score = train_score,\n",
    "        train_best_score = round(best_train_acc,2),\n",
    "        train_ground = train_ground,\n",
    "        train_predictions = train_predictions,\n",
    "        train_predicted_probabilities = np.array(train_predicted_probabilities),\n",
    "        valid_loss = valid_loss,\n",
    "        valid_score = valid_score,\n",
    "        valid_best_score = round(best_val_acc,2),\n",
    "        valid_ground = valid_ground,\n",
    "        valid_predictions = valid_predictions,\n",
    "        valid_predicted_probabilities =  np.array(valid_predicted_probabilities),\n",
    "        training_time = training_time\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_results(train_results, super_category):\n",
    "    \n",
    "    \n",
    "    standard_error = train_results['standard_error']\n",
    "    y_upper = train_results[\"valid_score\"] + standard_error\n",
    "    y_lower = train_results[\"valid_score\"] - standard_error\n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"Results\")\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(0,len(train_results[\"train_score\"])), train_results[\"train_score\"], label='train')\n",
    "\n",
    "    plt.plot(range(0,len(train_results[\"valid_score\"])), train_results[\"valid_score\"], label='valid')\n",
    "    \n",
    "    \n",
    "    kwargs = {'color': 'black', 'linewidth': 1, 'linestyle': '--', 'dashes':(5, 5)}\n",
    "    plt.plot(range(0,len(train_results[\"valid_score\"])), y_lower, **kwargs)\n",
    "    plt.plot(range(0,len(train_results[\"valid_score\"])), y_upper, **kwargs, label='validation SE (68% CI)')\n",
    "    \n",
    "    \n",
    "    plt.title('Accuracy Plot - ' + super_category, fontsize=20)\n",
    "    plt.ylabel('Accuracy', fontsize=16)\n",
    "    plt.xlabel('Training Epochs', fontsize=16)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(0,len(train_results[\"train_loss\"])), train_results[\"train_loss\"], label='train')\n",
    "    plt.plot(range(0,len(train_results[\"valid_loss\"])), train_results[\"valid_loss\"], label='valid')\n",
    "    \n",
    "\n",
    "    \n",
    "    plt.title('Loss Plot - ' + super_category, fontsize=20)\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.xlabel('Training Epochs', fontsize=16)\n",
    "    max_train_loss = max(train_results[\"train_loss\"])\n",
    "    max_valid_loss = max(train_results[\"valid_loss\"])\n",
    "    y_max_t_v = max_valid_loss if max_valid_loss > max_train_loss else max_train_loss\n",
    "    ylim_loss = y_max_t_v if y_max_t_v > 1 else 1\n",
    "    plt.ylim(0, ylim_loss)\n",
    "    plt.legend()\n",
    "\n",
    "  \n",
    "    plt.show()\n",
    "    \n",
    "    super_category_path = os.path.join(PREDICTIONS_PATH,super_category, \"train_results.png\")\n",
    "    fig.savefig(super_category_path, dpi=fig.dpi)\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_error_bar(best_score, valid_examples):\n",
    "    \n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"Standard Error\")\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "\n",
    "    \n",
    "    err = np.sqrt((best_score * (1-best_score))/valid_examples)\n",
    "    err_rounded_68 = round(err,2)\n",
    "    err_rounded_95 = round((err_rounded_68 * 2),2)\n",
    "   \n",
    "    print('Error (68% CI): +- ' + str(err_rounded_68))\n",
    "    print('Error (95% CI): +- ' + str(err_rounded_95))\n",
    "    print()\n",
    "    return err_rounded_68\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(grounds, preds, super_category, categories):\n",
    "    \n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    num_cat = []\n",
    "    for ind, cat in enumerate(categories):\n",
    "        print(\"Class {0} : {1}\".format(ind, cat))\n",
    "        num_cat.append(ind)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    \n",
    "    cm = confusion_matrix(grounds, preds, labels=num_cat)\n",
    "    \n",
    "    figsize = (10,8) if len(categories) <= 15 else (20,20)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot()\n",
    "    sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_title('Confusion Matrix - '+ super_category, fontsize=20)\n",
    "    ax.set_xlabel('Predicted labels', fontsize=16)\n",
    "    ax.set_ylabel('True labels', fontsize=16)\n",
    "    \n",
    "    ax.xaxis.set_ticklabels(num_cat)\n",
    "    ax.yaxis.set_ticklabels(num_cat)\n",
    "    \n",
    "    plt.pause(0.1)\n",
    "    super_category_path = os.path.join(PREDICTIONS_PATH,super_category, \"confusion_matrix.png\")\n",
    "    fig.savefig(super_category_path, dpi=fig.dpi)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Plot Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_images(super_category, categories, grounds, train_images,train_image_names):\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"Sample Images - \", super_category)\n",
    "    print(\"--------------------------------------------\")\n",
    "\n",
    "    \n",
    "    uniques, indexes = np.unique(grounds, return_index=True)\n",
    "    rows = math.ceil(len(indexes)/5)\n",
    "    fig = plt.figure(figsize=(30,7*rows))\n",
    "    \n",
    "    k=0\n",
    "    for i in range(0, len(indexes)):\n",
    "        fig.add_subplot(rows,5,k+1)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        lbl_alph = categories[uniques[i]]\n",
    "        lbl_int = str(uniques[i])\n",
    "        lbl_file = train_image_names[uniques[i]]\n",
    "        \n",
    "        title_lbl = \"\\n\".join(wrap(\"{}({})\".format(lbl_alph,lbl_int),30))\n",
    "        title_file = \"\\n\".join(wrap(lbl_file,30))\n",
    "        title = title_lbl+\"\\n\"+title_file\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.imshow(train_images[indexes[i]])\n",
    "        k += 1\n",
    "    plt.pause(0.1)\n",
    "    print()   \n",
    "    \n",
    "    super_category_path = os.path.join(PREDICTIONS_PATH,super_category, \"sample_images.png\")\n",
    "    fig.savefig(super_category_path, dpi=fig.dpi, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Wrongly Classified Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wrongly_classified_images(super_category, categories, grounds, preds, valid_images, valid_image_names):\n",
    "    \n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"Wrongly Classified Images - \", super_category)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(30,20))\n",
    "    k=0\n",
    "    \n",
    "    for i in range(0, len(grounds)):\n",
    "        if grounds[i] != preds[i]:\n",
    "            \n",
    "           \n",
    "            fig.add_subplot(1,5,k+1)\n",
    "            plt.axis('off')\n",
    "            \n",
    "            \n",
    "            orig_lbl_alph = categories[grounds[i]]\n",
    "            orig_lbl_int = str(grounds[i])\n",
    "           \n",
    "            \n",
    "            pred_lbl_alph = categories[preds[i]]\n",
    "            pred_lbl_int = str(preds[i])\n",
    "            pred_lbl_file = valid_image_names[preds[i]]\n",
    "            \n",
    "            org_title_lbl = \"\\n\".join(wrap(\"Orig lbl : {}({})\".format(orig_lbl_alph,orig_lbl_int),30))\n",
    "            pred_title_lbl = \"\\n\".join(wrap(\"Pred lbl : {}({})\".format(pred_lbl_alph,pred_lbl_int),30))\n",
    "            pred_title_file = \"\\n\".join(wrap(pred_lbl_file,30))\n",
    "            \n",
    "            title = org_title_lbl+\"\\n\"+pred_title_lbl+\"\\n\"+pred_title_file\n",
    "            \n",
    "           \n",
    "            plt.title(title)\n",
    "            plt.imshow(valid_images[i])\n",
    "            k += 1\n",
    "        if k == 5:\n",
    "            break\n",
    "    plt.pause(0.1)\n",
    "    print()\n",
    "    \n",
    "    super_category_path = os.path.join(PREDICTIONS_PATH,super_category, \"wrongly_classified_images.png\")\n",
    "    fig.savefig(super_category_path, dpi=fig.dpi, bbox_inches='tight')\n",
    "    \n",
    "def get_wrongly_classified_images_indexes(grounds, preds):\n",
    "    wrong_images_indexes = []\n",
    "    for i in range(0, len(grounds)):\n",
    "        if grounds[i] != preds[i]:\n",
    "            wrong_images_indexes.append(i)\n",
    "    return wrong_images_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autodl_auc(outputs, targets, predictions):\n",
    "    \n",
    "    targets = np.asarray(targets)\n",
    "    predictions = np.asarray(predictions)\n",
    "    \n",
    "    numclass = targets.max()+1\n",
    "    \n",
    "    boolean_array = np.zeros((len(outputs),numclass), dtype=bool)\n",
    "    \n",
    "    for labelindex in range(numclass):\n",
    "        boolean_array[:,labelindex]= (targets == labelindex)\n",
    "    \n",
    "    \n",
    "    auc = roc_auc_score(boolean_array, outputs)\n",
    "    auc_1 = 2*auc-1\n",
    "    \n",
    "    \n",
    "    return round(auc, 2), round(auc_1, 2)\n",
    "   \n",
    "\n",
    "\n",
    "def plot_auc(outputs, targets, predictions, autodl_auc_0,autodl_auc_1, super_category, categories):\n",
    "    \n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"Average AUC\")\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    print(\"AUC : \", autodl_auc_0)\n",
    "    print(\"2*AUC-1 : \", autodl_auc_1)\n",
    "    print()\n",
    "    \n",
    "\n",
    "    \n",
    "    targets = np.asarray(targets)\n",
    "    predictions = np.asarray(predictions)\n",
    "    \n",
    "    \n",
    "    \n",
    "    numclass = len(categories) #targets.max()+1 \n",
    "    \n",
    "    scores_auroc = []\n",
    "    scores_auroc_1 =[]\n",
    "\n",
    "    for labelindex in range(numclass):\n",
    "        binary_targets = (targets == labelindex)\n",
    "        binary_predictions = (predictions == labelindex)\n",
    "       \n",
    "        selected_outputs = outputs[:,labelindex]\n",
    "  \n",
    "        auroc = roc_auc_score(binary_targets, selected_outputs)\n",
    "        \n",
    "        scores_auroc.append(auroc)\n",
    "        scores_auroc_1.append(2*auroc-1)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #save categories auc  \n",
    "    categories_auc_textfile_path = os.path.join(PREDICTIONS_PATH, super_category, 'categories_auc.txt') \n",
    "    with open(categories_auc_textfile_path, 'w', encoding=\"utf-8\") as f:\n",
    "        for i,cat in enumerate(categories):\n",
    "            single_auroc = round(scores_auroc[i], 2)\n",
    "            \n",
    "            f.write(\"%s : %s\\n\" %(cat,single_auroc))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    average_auc = round(np.mean(scores_auroc), 2)\n",
    "    average_auc_1 = round(np.mean(scores_auroc_1), 2)\n",
    "    if average_auc_1 == 0.0:\n",
    "        average_auc_1 = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot AUC Score\n",
    "    ymin = np.min(scores_auroc_1) if np.min(scores_auroc_1) < 0  else 0\n",
    "    width = 0.2\n",
    "    \n",
    "   \n",
    "    fig_height = 8 if len(categories) <= 5 else 16\n",
    "    fig = plt.figure(figsize=(3*numclass,fig_height))\n",
    "    \n",
    "    \n",
    "    plt.bar(np.arange(numclass), scores_auroc, width,  label='AUC')\n",
    "    plt.bar(np.arange(numclass)+width, scores_auroc_1, width, label='2*AUC-1')\n",
    "    plt.hlines(y=0.0, xmin=-width, xmax=numclass-1+width*2, linewidth=1, linestyles='-', color='black')\n",
    "    plt.hlines(y=average_auc, xmin=-width, xmax=numclass-1+width*2, linewidth=2, linestyles='--', color='b', label='Average AUC : %0.2f'%average_auc)\n",
    "    plt.hlines(y=average_auc_1, xmin=-width, xmax=numclass-1+width*2, linewidth=2, linestyles='--', color='r', label='Average 2*AUC-1 : %0.2f'%average_auc_1)\n",
    "    plt.title('AUC Score - ' + super_category, fontsize=20)\n",
    "    plt.ylabel('AUC Score', fontsize=16)\n",
    "    plt.xlabel('Classes', fontsize=16)\n",
    "    plt.ylim(ymin,1)\n",
    "    plt.xticks(np.arange(numclass) + width / 2, np.arange(numclass))\n",
    "    plt.legend()\n",
    "#     plt.show()\n",
    "    plt.pause(0.1)\n",
    "    \n",
    "    super_category_path = os.path.join(PREDICTIONS_PATH,super_category, \"auc.png\")\n",
    "    fig.savefig(super_category_path, dpi=fig.dpi)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # histogram\n",
    "    fig = plt.figure(figsize=(8,5))\n",
    "    title = 'AUC Histogram - '+super_category\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.xlabel('AUC Score', fontsize=16)\n",
    "    plt.ylabel('Frequency', fontsize=16)\n",
    "    plt.hist(scores_auroc, alpha=0.5, ec='black')\n",
    "    plt.pause(0.1)\n",
    "    super_category_path = os.path.join(PREDICTIONS_PATH,super_category, \"auc_histogram.png\")\n",
    "    fig.savefig(super_category_path, dpi=fig.dpi)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(train_outputs, train_targets, train_predictions, valid_outputs, valid_targets, valid_predictions, super_category):\n",
    "    \n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"ROC Curves\")\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    \n",
    "   \n",
    "    train_targets = np.asarray(train_targets)\n",
    "    train_predictions = np.asarray(train_predictions)\n",
    "    valid_targets = np.asarray(valid_targets)\n",
    "    valid_predictions = np.asarray(valid_predictions)\n",
    "    \n",
    "    \n",
    "    numclass = train_predictions.max()+1 \n",
    "    \n",
    "    \n",
    "    train_auc_curves = []\n",
    "    valid_auc_curves = []\n",
    "\n",
    "    for labelindex in range(numclass):\n",
    "        train_binary_targets = (train_targets == labelindex)\n",
    "        train_binary_predictions = (train_predictions == labelindex)\n",
    "        \n",
    "        valid_binary_targets = (valid_targets == labelindex)\n",
    "        valid_binary_predictions = (valid_predictions == labelindex)\n",
    "       \n",
    "        selected_train_outputs = train_outputs[:,labelindex]\n",
    "        selected_valid_outputs = valid_outputs[:,labelindex]\n",
    "        \n",
    "        train_fpr, train_tpr, _ = roc_curve(train_binary_targets, selected_train_outputs)\n",
    "        valid_fpr, valid_tpr, _ = roc_curve(valid_binary_targets, selected_valid_outputs)\n",
    "       \n",
    "        train_auc_curves.append([train_fpr,train_tpr])\n",
    "        valid_auc_curves.append([valid_fpr,valid_tpr])\n",
    "        \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "     # Plot ROC Curves\n",
    "\n",
    "    fig_height = 8 if numclass <= 5 else 16\n",
    "    fig_width =  16 if numclass <= 5 else 30\n",
    "    fig = plt.figure(figsize=(fig_width,fig_height))\n",
    "   \n",
    "\n",
    "   \n",
    "    plt.subplot(1,2,1)\n",
    "    for idx, auc_cur in enumerate(train_auc_curves): \n",
    "        plt.plot(auc_cur[0], auc_cur[1], marker='.',  label='Class:'+str(idx))\n",
    "    plt.plot([0,1], [0,1], linestyle='--', color='black')\n",
    "    plt.title('Train ROC Curves - ' + super_category, fontsize=20)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    for idx, auc_cur in enumerate(valid_auc_curves): \n",
    "        plt.plot(auc_cur[0], auc_cur[1], marker='.',  label='Class:'+str(idx))\n",
    "    plt.plot([0,1], [0,1], linestyle='--', color='black')\n",
    "    plt.title('Valid ROC Curves - ' + super_category, fontsize=20)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.pause(0.1)\n",
    "    \n",
    "    super_category_path = os.path.join(PREDICTIONS_PATH,super_category, \"roc_curves.png\")\n",
    "    fig.savefig(super_category_path, dpi=fig.dpi)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(super_category, categories, training_result):\n",
    "\n",
    "    train_dic_for_df = {}\n",
    "    valid_dic_for_df = {}\n",
    "\n",
    "\n",
    "    train_dic_for_df['ground_truth'] = training_result['train_ground']\n",
    "    train_dic_for_df['predictions'] = training_result['train_predictions']\n",
    "    \n",
    "    valid_dic_for_df['ground_truth'] = training_result['valid_ground']\n",
    "    valid_dic_for_df['predictions'] = training_result['valid_predictions']\n",
    "\n",
    "    train_probability_array = list(training_result['train_predicted_probabilities'].T)\n",
    "    valid_probability_array = list(training_result['valid_predicted_probabilities'].T)\n",
    "    for idx, prob in enumerate(train_probability_array):\n",
    "        key = 'prob_'+str(idx)\n",
    "        train_dic_for_df[key] = prob\n",
    "    for idx, prob in enumerate(valid_probability_array):\n",
    "        key = 'prob_'+str(idx)\n",
    "        valid_dic_for_df[key] = prob\n",
    "\n",
    "\n",
    "    train_df = pd.DataFrame(train_dic_for_df)\n",
    "    valid_df = pd.DataFrame(valid_dic_for_df)\n",
    "    \n",
    "\n",
    "    \n",
    "    csv_train = os.path.join(PREDICTIONS_PATH, super_category, 'train.csv') \n",
    "    csv_valid = os.path.join(PREDICTIONS_PATH, super_category, 'valid.csv') \n",
    "    \n",
    "\n",
    "    #save train valid_predictions_ground_probabilities in CSV  \n",
    "    train_df.to_csv(csv_train, index=False)\n",
    "    valid_df.to_csv(csv_valid, index=False)\n",
    "\n",
    "    \n",
    "    #save categories  \n",
    "    categories_textfile_path = os.path.join(PREDICTIONS_PATH, super_category, 'categories.txt') \n",
    "    with open(categories_textfile_path, 'w', encoding=\"utf-8\") as f:\n",
    "        for item in categories:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    \n",
    "    #save category logs\n",
    "    category_logfile_path = os.path.join(PREDICTIONS_PATH, super_category, 'logs.txt') \n",
    "    with open(category_logfile_path, 'w') as f:\n",
    "        f.write(\"Total Images : %s\\n\" % training_result['total_images'])\n",
    "        f.write(\"Training Images : %s\\n\" % training_result['train_images'])\n",
    "        f.write(\"Validation Images : %s\\n\" % training_result['valid_images'])\n",
    "        f.write(\"Training Time : %s\\n\" % training_result['training_time'])\n",
    "        f.write(\"Best Train Accuracy : %s\\n\" % training_result['train_best_score'])\n",
    "        f.write(\"Best Valid Accuracy : %s\\n\" % training_result['valid_best_score'])\n",
    "        f.write(\"AUC : %s\\n\" % training_result['AUC'])\n",
    "        f.write(\"2*AUC-1 : %s\\n\" % training_result['AUC_1'])\n",
    "        f.write(\"Standard Error : %s\\n\" % training_result['standard_error'])\n",
    "        \n",
    "\n",
    "    \n",
    "    print(\"Saved Results for Super-Category : \", super_category)\n",
    "    print()\n",
    "    print()\n",
    "        \n",
    "    #save super-category\n",
    "    super_categories_textfile_path = os.path.join(PREDICTIONS_PATH, 'super_categories.txt') \n",
    "    with open(super_categories_textfile_path, 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(\"%s\\n\" % super_category)\n",
    "        \n",
    "        \n",
    "def save_overall_logs():\n",
    "    #save logs \n",
    "    overall_logsfile_path = os.path.join(PREDICTIONS_PATH, 'logs.txt') \n",
    "    with open(overall_logsfile_path, 'w') as f:\n",
    "        f.write(\"Total Super Categories : %s\\n\" % total_super_categories)\n",
    "        f.write(\"Total Categories : %s\\n\" % total_categories)\n",
    "        f.write(\"Iterations Needed : %s\\n\" % iterations_needed)\n",
    "        f.write(\"Classes to Combine : %s\\n\" % CATEGORIES_TO_COMBINE)\n",
    "        \n",
    "    print(\"#################################\")\n",
    "    print(\"Saved Overall logs of experiment\")\n",
    "    print(\"#################################\")\n",
    "        \n",
    "        \n",
    "        \n",
    "def make_super_category_directory(super_category):\n",
    "    \n",
    "    super_category_path = os.path.join(PREDICTIONS_PATH,super_category)\n",
    "    if not os.path.exists(super_category_path):\n",
    "        os.makedirs(super_category_path)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate overall histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_range(dictionary, begin, end):\n",
    "    return dict(e for i, e in enumerate(dictionary.items()) if begin <= i <= end)\n",
    "\n",
    "\n",
    "def generate_overall_auc_histogram_and_desc_auc_plot():\n",
    "    \n",
    "    #Read super_Categories\n",
    "    super_categories_textfile_path = os.path.join(PREDICTIONS_PATH, 'super_categories.txt') \n",
    "    with open(super_categories_textfile_path, 'r') as f:\n",
    "        super_categories = f.read().splitlines()\n",
    "        \n",
    "    #Read Categories AUC    \n",
    "    categories_dic = {}\n",
    "    for sup_cat in super_categories:\n",
    "        categories_auc_textfile_path = os.path.join(PREDICTIONS_PATH, sup_cat, 'categories_auc.txt') \n",
    "        with open(categories_auc_textfile_path, 'r') as f:\n",
    "            categories_auc = f.read().splitlines()\n",
    "        for item in categories_auc:\n",
    "            arr_split = item.split(' : ')\n",
    "            categories_dic[arr_split[0]] = float(arr_split[1])\n",
    "            \n",
    "    #Sort in descending order        \n",
    "    sorted_categories_dic = dict(sorted(categories_dic.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    \n",
    "    print(\"##########################################\")\n",
    "    print(\"Saving Overall AUC Histogram\")\n",
    "    print(\"##########################################\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    #plot histogram\n",
    "    fig = plt.figure(figsize=(15,8))\n",
    "    plt.title('All CategoriesAUC Histogram', fontsize=20)\n",
    "    plt.xlabel('AUC Score', fontsize=16)\n",
    "    plt.ylabel('Frequency', fontsize=16)\n",
    "    plt.hist(list(sorted_categories_dic.values()), alpha=0.5, ec='black')\n",
    "    plt.show()\n",
    "    overall_categoris_auc_histogram_path = os.path.join(PREDICTIONS_PATH, \"overall_auc_histogram.png\")\n",
    "    fig.savefig(overall_categoris_auc_histogram_path, dpi=fig.dpi)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"##########################################\")\n",
    "    print(\"Saving Desc AUC Plot\")\n",
    "    print(\"##########################################\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    total_cat = len(sorted_categories_dic)\n",
    "    cat_per_plot = 30\n",
    "    plots_needed = math.ceil(len(sorted_categories_dic)/cat_per_plot)\n",
    "\n",
    "    print(\"Total Categories : \", total_cat)\n",
    "    print(\"Plots Needed : \", plots_needed)\n",
    "    print(\"Categories per plot : \", cat_per_plot)\n",
    "    \n",
    "    fig, axs = plt.subplots(plots_needed,1, figsize=(20, plots_needed*cat_per_plot))\n",
    "    fig.subplots_adjust(hspace = .1, wspace=.001)\n",
    "    \n",
    "    if type(axs) is np.ndarray:\n",
    "        axs = axs.ravel()\n",
    "    else:\n",
    "        axs = [axs]\n",
    "\n",
    "    for i in range(plots_needed):\n",
    "        begin = i*cat_per_plot\n",
    "        end = (i*cat_per_plot+cat_per_plot-1) if (i*cat_per_plot+cat_per_plot-1) < total_cat else total_cat \n",
    "        slice_i = get_range(sorted_categories_dic, begin, end)\n",
    "\n",
    "        axs[i].barh(list(slice_i.keys()), list(slice_i.values()))\n",
    "        axs[i].set_title(\"All Categories AUC - Slice \"+ str(i+1), fontsize=20)\n",
    "        axs[i].xaxis.set_tick_params(rotation=90)\n",
    "        axs[i].set_xlabel(\"AUC Score\")\n",
    "        axs[i].set_ylabel(\"Category\")\n",
    "        axs[i].invert_yaxis()\n",
    "        ymin,ymax=axs[i].get_ylim()\n",
    "        axs[i].vlines(0.5, ymin=ymin, ymax=ymax, linestyles =\"--\", colors =\"r\")\n",
    "\n",
    "\n",
    "    descending_categoris_auc_path = os.path.join(PREDICTIONS_PATH, \"descending_auc.png\")\n",
    "    fig.savefig(descending_categoris_auc_path, dpi=fig.dpi)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Image Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_sheet():\n",
    "    \n",
    "    print(\"#################################\")\n",
    "    print(\"Generating Imagesheet\")\n",
    "    print(\"#################################\")\n",
    "    \n",
    "    rows = math.ceil(IMAGES_PER_CATEGORY/5)\n",
    "    imagesheet_path = PREDICTIONS_PATH +'/imagesheet.pdf'\n",
    "    with PdfPages(imagesheet_path) as pdf:\n",
    "        \n",
    "        for cat in categories:\n",
    "            category_images = data[data[CATEGORY_COLUMN] == cat].groupby(CATEGORY_COLUMN).sample(n=IMAGES_PER_CATEGORY, random_state=SEED)[IMAGE_COLUMN].values\n",
    "            fig = plt.figure(figsize=(30,5*rows))\n",
    "            k=0\n",
    "            for i, image_name in enumerate(category_images):\n",
    "                fig.add_subplot(rows,5,k+1)\n",
    "                \n",
    "                file = IMAGE_PATH+\"/\"+image_name\n",
    "                img = cv2.imread(file)\n",
    "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                title = \"\\n\".join(wrap(image_name,30))\n",
    "              \n",
    "                \n",
    "                plt.title(title)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(img_rgb)\n",
    "                \n",
    "                k += 1\n",
    "\n",
    "            fig.suptitle(\"\\n\"+cat, fontsize=40)\n",
    "            pdf.savefig(fig)\n",
    "            plt.pause(0.1)\n",
    "    \n",
    "            plt.close(fig)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magic Happens here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(single_super_data):\n",
    "        \n",
    "    train_images = []\n",
    "    valid_images = []\n",
    "    \n",
    "    for image_name in single_super_data['train_data']:\n",
    "        file = IMAGE_PATH+\"/\"+image_name\n",
    "        img = cv2.imread(file)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        train_images.append(img_rgb)\n",
    "\n",
    "    for image_name in single_super_data['valid_data']:\n",
    "        file = IMAGE_PATH+\"/\"+image_name\n",
    "        img = cv2.imread(file)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        valid_images.append(img_rgb)\n",
    "        \n",
    "    single_super_data['train_images'] = train_images\n",
    "    single_super_data['valid_images'] = valid_images\n",
    "    return single_super_data\n",
    "\n",
    "\n",
    "\n",
    "def train_single_super_category(single_super_data):\n",
    "    \n",
    "    single_super_data = load_images(single_super_data)\n",
    "    \n",
    "\n",
    "    dataloaders, dataset_sizes, data_stats = make_dataset(single_super_data)\n",
    "    model = getModel(number_of_classes=len(single_super_data['categories']))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    result = train_model(model, dataloaders, dataset_sizes, criterion, optimizer, exp_lr_scheduler, num_epochs=10)\n",
    "    \n",
    "    \n",
    "    #data statistics\n",
    "    result['total_images'] = data_stats['total_images']\n",
    "    result['train_images'] = data_stats['train_images']\n",
    "    result['valid_images'] = data_stats['valid_images']\n",
    "    \n",
    "    \n",
    "    \n",
    "    # make directory for storing results\n",
    "    make_super_category_directory(single_super_data['super_category'])\n",
    "\n",
    "    #Accuracy, Loss and Error\n",
    "    result['standard_error'] = get_error_bar(result['valid_best_score'], len(single_super_data['valid_images']))\n",
    "    plot_train_results(result, single_super_data['super_category'])\n",
    "    \n",
    "        \n",
    "    #Confusion Matrix\n",
    "    plot_confusion_matrix(result['valid_ground'], result['valid_predictions'], single_super_data['super_category'], single_super_data['categories'])\n",
    "    \n",
    "    #AUC\n",
    "    autodl_auc_0 , autodl_auc_1 = autodl_auc(result['valid_predicted_probabilities'], result['valid_ground'], result['valid_predictions'])\n",
    "    result['AUC'] = autodl_auc_0\n",
    "    result['AUC_1'] = autodl_auc_1\n",
    "    plot_auc(result['valid_predicted_probabilities'], result['valid_ground'], result['valid_predictions'],autodl_auc_0, autodl_auc_1, single_super_data['super_category'], single_super_data['categories'])\n",
    "    \n",
    "    \n",
    "    #ROC Curves\n",
    "    plot_roc_curves(result['train_predicted_probabilities'], result['train_ground'], result['train_predictions'],\n",
    "            result['valid_predicted_probabilities'], result['valid_ground'], result['valid_predictions']\n",
    "            , single_super_data['super_category'])\n",
    "    \n",
    "    #Sample Images\n",
    "    plot_sample_images(single_super_data['super_category'], single_super_data['categories'], result['train_ground'], single_super_data['train_images'], single_super_data['train_data'])\n",
    "    \n",
    "    #Wrong Images\n",
    "    plot_wrongly_classified_images(single_super_data['super_category'], single_super_data['categories'], result['valid_ground'], result['valid_predictions'], single_super_data['valid_images'], single_super_data['valid_data'])\n",
    "    \n",
    "    #Save Predictions\n",
    "    save_predictions(single_super_data['super_category'], single_super_data['categories'], result)\n",
    "\n",
    "    \n",
    "    \n",
    "    #CleanUp\n",
    "    del dataloaders\n",
    "    del model\n",
    "    del criterion\n",
    "    del optimizer\n",
    "    del exp_lr_scheduler\n",
    "    \n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save overall logs\n",
    "save_overall_logs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate image sheet\n",
    "if GENERATE_IMAGESHEET:\n",
    "    generate_image_sheet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all random super-categories to get results per super-category\n",
    "for index, singlee_super_data in enumerate(super_data):\n",
    "    if MAX_EPISODES is None or index < MAX_EPISODES:\n",
    "        super_results[singlee_super_data['super_category']] = train_single_super_category(singlee_super_data)\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate over all hostogram and auc desc plot\n",
    "generate_overall_auc_histogram_and_desc_auc_plot()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The whole process done in {:.2f} s.\".format(time.time() - t0_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
